#!/usr/bin/env python3
"""
FlatQuant æ•°æ®é›†ä¸‹è½½è„šæœ¬
æ ¹æ®README.mdè¦æ±‚ä¸‹è½½æ‰€æœ‰å¿…è¦çš„æ•°æ®é›†
"""

import os
import sys
import json
import shutil
from pathlib import Path
from datasets import load_dataset
import logging
from datetime import datetime

# è®¾ç½®æ—¥å¿—
def setup_logging():
    """è®¾ç½®æ—¥å¿—è®°å½•"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler(f'dataset_download_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log')
        ]
    )
    return logging.getLogger(__name__)

logger = setup_logging()

# æ•°æ®é›†é…ç½®
DATASETS_CONFIG = {
    # æ ¡å‡†å’Œå›°æƒ‘åº¦è¯„ä¼°æ•°æ®é›†
    "calibration_datasets": {
        # "wikitext": {
        #     "name": "wikitext",
        #     "config": "wikitext-2-raw-v1",
        #     "local_dir": "./datasets/wikitext",
        #     "url": "https://huggingface.co/datasets/wikitext",
        #     "expected_splits": ["train", "validation"]
        # },
        "c4": {
            "name": "allenai/c4",
            "config": "en",
            "local_dir": "./datasets/allenai/c4",
            "url": "https://huggingface.co/datasets/allenai/c4",
            "expected_splits": ["train", "validation"]
        },
        # "pile": {
        #     "name": "mit-han-lab/pile-val-backup",
        #     "config": None,
        #     "local_dir": "./datasets/pile-val-backup",
        #     "url": "https://huggingface.co/datasets/mit-han-lab/pile-val-backup",
        #     "expected_splits": ["validation"]
        # }
    },
    
    # å¸¸è¯†é—®ç­”è¯„ä¼°æ•°æ®é›†
    "qa_datasets": {
        "ai2_arc": {
            "name": "allenai/ai2_arc",
            "config": ["ARC-Challenge", "ARC-Easy"],
            "local_dir": "./datasets/ai2_arc",
            "url": "https://huggingface.co/datasets/allenai/ai2_arc",
            "expected_splits": ["train", "validation", "test"]
        },
        "hellaswag": {
            "name": "Rowan/hellaswag",
            "config": None,
            "local_dir": "./datasets/hellaswag",
            "url": "https://huggingface.co/datasets/Rowan/hellaswag",
            "expected_splits": ["train", "validation"]
        },
        "lambada_openai": {
            "name": "EleutherAI/lambada_openai",
            "config": None,
            "local_dir": "./datasets/lambada_openai",
            "url": "https://huggingface.co/datasets/EleutherAI/lambada_openai",
            "expected_splits": ["test"]
        },
        "piqa": {
            "name": "ybisk/piqa",
            "config": None,
            "local_dir": "./datasets/piqa",
            "url": "https://huggingface.co/datasets/ybisk/piqa",
            "expected_splits": ["train", "validation"]
        },
        "winogrande": {
            "name": "winogrande",
            "config": "winogrande_xl",
            "local_dir": "./datasets/winogrande",
            "url": "https://huggingface.co/datasets/winogrande",
            "expected_splits": ["train", "validation"]
        }
    }
}

def create_directories():
    """åˆ›å»ºå¿…è¦çš„ç›®å½•ç»“æ„"""
    directories = [
        "./datasets",
        "./datasets/wikitext",
        "./datasets/allenai/c4",
        "./datasets/pile-val-backup",
        "./datasets/ai2_arc",
        "./datasets/hellaswag",
        "./datasets/lambada_openai",
        "./datasets/piqa",
        "./datasets/winogrande",
        "./datasets/lm_eval_configs",
        "./datasets/lm_eval_configs/tasks"
    ]
    
    for directory in directories:
        Path(directory).mkdir(parents=True, exist_ok=True)
        logger.info(f"åˆ›å»ºç›®å½•: {directory}")

# def check_dataset_completeness(dataset_info):
#     """æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å®Œæ•´ä¸‹è½½"""
#     try:
#         dataset_dir = Path(dataset_info['local_dir'])
        
#         # å¦‚æœç›®å½•ä¸å­˜åœ¨ï¼Œè¯´æ˜æœªå¼€å§‹ä¸‹è½½
#         if not dataset_dir.exists():
#             return "not_started"
        
#         # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®é›†æ–‡ä»¶
#         dataset_files = list(dataset_dir.rglob("*.arrow")) + list(dataset_dir.rglob("*.parquet")) + list(dataset_dir.rglob("dataset_info.json"))
        
#         if not dataset_files:
#             return "incomplete"
        
#         # å°è¯•åŠ è½½æ•°æ®é›†éªŒè¯å®Œæ•´æ€§
#         try:
#             dataset = load_dataset(dataset_info['local_dir'])
            
#             # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
#             total_samples = 0
#             for split_name, split_data in dataset.items():
#                 total_samples += len(split_data)
            
#             if total_samples == 0:
#                 return "incomplete"
            
#             # æ£€æŸ¥é¢„æœŸçš„åˆ†å‰²æ˜¯å¦å­˜åœ¨
#             expected_splits = dataset_info.get('expected_splits', [])
#             if expected_splits:
#                 available_splits = list(dataset.keys())
#                 missing_splits = set(expected_splits) - set(available_splits)
#                 if missing_splits:
#                     logger.warning(f"æ•°æ®é›†ç¼ºå°‘åˆ†å‰²: {missing_splits}")
#                     return "incomplete"
            
#             return "complete"
            
#         except Exception as e:
#             logger.warning(f"æ•°æ®é›†åŠ è½½å¤±è´¥ï¼Œå¯èƒ½ä¸å®Œæ•´: {str(e)}")
#             return "incomplete"
            
#     except Exception as e:
#         logger.warning(f"æ£€æŸ¥æ•°æ®é›†å®Œæ•´æ€§æ—¶å‡ºé”™: {str(e)}")
#         return "incomplete"

def check_specific_config_completeness(dataset_info, config):
    """æ£€æŸ¥ç‰¹å®šé…ç½®çš„å­æ•°æ®é›†æ˜¯å¦å®Œæ•´"""
    try:
        dataset_dir = Path(dataset_info['local_dir'])
        
        # æ„å»ºç‰¹å®šé…ç½®çš„å­ç›®å½•è·¯å¾„
        if config:
            config_dir = dataset_dir / config
            if not config_dir.exists():
                return False
        else:
            config_dir = dataset_dir
        
        # æ£€æŸ¥è¯¥é…ç½®ä¸‹æ˜¯å¦æœ‰æ•°æ®æ–‡ä»¶
        data_files = list(config_dir.rglob("*.arrow")) + list(config_dir.rglob("*.parquet"))
        if not data_files:
            return False
        
        # å°è¯•åŠ è½½è¯¥é…ç½®çš„æ•°æ®é›†
        load_kwargs = {}
        if config:
            load_kwargs["name"] = config
            
        dataset = load_dataset(
            dataset_info['name'],
            cache_dir=str(dataset_dir),** load_kwargs
        )
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
        total_samples = 0
        for split_name, split_data in dataset.items():
            total_samples += len(split_data)
        
        if total_samples == 0:
            return False
        
        # æ£€æŸ¥é¢„æœŸçš„åˆ†å‰²æ˜¯å¦å­˜åœ¨
        expected_splits = dataset_info.get('expected_splits', [])
        if expected_splits:
            available_splits = list(dataset.keys())
            missing_splits = set(expected_splits) - set(available_splits)
            if missing_splits:
                logger.warning(f"é…ç½® {config} ç¼ºå°‘åˆ†å‰²: {missing_splits}")
                return False
        
        return True
        
    except Exception as e:
        logger.warning(f"æ£€æŸ¥é…ç½® {config} å®Œæ•´æ€§æ—¶å‡ºé”™: {str(e)}")
        return False

def download_dataset(dataset_info, dataset_type, force=False):
    """ä¸‹è½½å•ä¸ªæ•°æ®é›†ï¼ˆæ”¯æŒå¤šé…ç½®ï¼‰"""
    try:
        logger.info(f"å¼€å§‹å¤„ç† {dataset_type} æ•°æ®é›†: {dataset_info['name']}")
        
        # ç¡®ä¿configå§‹ç»ˆæ˜¯å¯è¿­ä»£å¯¹è±¡ï¼Œä¿®å¤å˜é‡æœªèµ‹å€¼é—®é¢˜
        config_value = dataset_info.get('config')
        if isinstance(config_value, list):
            configs = config_value
        elif config_value is not None:
            configs = [config_value]
        else:
            configs = [None]  # æ˜ç¡®è®¾ç½®ä¸ºåŒ…å«Noneçš„åˆ—è¡¨ï¼Œç¡®ä¿configæœ‰å€¼
        
        all_success = True
        
        for config in configs:
            # æ£€æŸ¥å½“å‰é…ç½®æ˜¯å¦å·²å®Œæ•´ä¸‹è½½
            is_complete = check_specific_config_completeness(dataset_info, config)
            
            # å¼ºåˆ¶é‡æ–°ä¸‹è½½æˆ–ä¸å®Œæ•´æ—¶æ‰ä¸‹è½½
            if force or not is_complete:
                if force and Path(dataset_info['local_dir']).exists():
                    # åªåˆ é™¤å½“å‰é…ç½®çš„ç›®å½•ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    if config:
                        config_dir = Path(dataset_info['local_dir']) / config
                        if config_dir.exists():
                            logger.info(f"ğŸ”„ å¼ºåˆ¶åˆ é™¤é…ç½® {config} çš„ç°æœ‰æ•°æ®")
                            shutil.rmtree(config_dir)
                
                logger.info(f"ğŸ“¥ å¼€å§‹ä¸‹è½½é…ç½® {config or 'default'}")
                
                # ä¸‹è½½å½“å‰é…ç½®
                download_kwargs = {"trust_remote_code": True}
                if config:
                    download_kwargs["name"] = config
                
                try:
                    dataset = load_dataset(
                        dataset_info['name'],
                        cache_dir=dataset_info['local_dir'],
                        **download_kwargs
                    )
                    
                    logger.info(f"âœ… æˆåŠŸä¸‹è½½é…ç½® {config or 'default'}")
                    for split_name, split_data in dataset.items():
                        logger.info(f"   - {split_name}: {len(split_data)} æ ·æœ¬")
                        
                except Exception as e:
                    logger.error(f"âŒ ä¸‹è½½é…ç½® {config or 'default'} å¤±è´¥: {str(e)}")
                    all_success = False
            else:
                logger.info(f"âœ… é…ç½® {config or 'default'} å·²å®Œæ•´ï¼Œè·³è¿‡ä¸‹è½½")
        
        return all_success
        
    except Exception as e:
        logger.error(f"âŒ å¤„ç†æ•°æ®é›† {dataset_info['name']} æ—¶å‡ºé”™: {str(e)}")
        return False

def setup_lm_eval_configs():
    """è®¾ç½®lm_evalé…ç½®æ–‡ä»¶"""
    try:
        logger.info("è®¾ç½®lm_evalé…ç½®æ–‡ä»¶...")
        
        # æŸ¥æ‰¾lm_evalå®‰è£…è·¯å¾„
        import lm_eval
        lm_eval_path = Path(lm_eval.__file__).parent / "tasks"
        
        if not lm_eval_path.exists():
            logger.warning(f"æœªæ‰¾åˆ°lm_eval tasksç›®å½•: {lm_eval_path}")
            return False
        
        # å¤åˆ¶é…ç½®æ–‡ä»¶
        target_path = Path("/data/disk1/FlatQuant-main/datasets/lm_eval_configs/tasks")
        target_path.mkdir(parents=True, exist_ok=True)
        
        # éœ€è¦å¤åˆ¶çš„é…ç½®æ–‡ä»¶
        config_files = [
            "arc/arc_easy.yaml",
            "arc/arc_challenge.yaml", 
            "hellaswag/hellaswag.yaml",
            "lambada/lambada_openai.yaml",
            "piqa/piqa.yaml",
            "winogrande/default.yaml"
        ]

        copied_count = 0
        for config_file in config_files:
            source_file = lm_eval_path / config_file
            target_file = target_path / config_file
            print("source_file", source_file)
            print("target_file", target_file)
            target_file.parent.mkdir(parents=True, exist_ok=True)
            if source_file.exists():
                shutil.copy2(source_file, target_file)
                logger.info(f"å¤åˆ¶é…ç½®æ–‡ä»¶: {config_file}")
                copied_count += 1
            else:
                logger.warning(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {source_file}")
        
        if copied_count > 0:
            logger.info(f"âœ… æˆåŠŸå¤åˆ¶ {copied_count} ä¸ªlm_evalé…ç½®æ–‡ä»¶")
            
            # ä¿®æ”¹é…ç½®æ–‡ä»¶ä¸­çš„æ•°æ®é›†è·¯å¾„
            modify_lm_eval_configs()
            return True
        else:
            logger.warning("æœªæ‰¾åˆ°ä»»ä½•lm_evalé…ç½®æ–‡ä»¶")
            return False
            
    except Exception as e:
        logger.error(f"âŒ è®¾ç½®lm_evalé…ç½®æ–‡ä»¶å¤±è´¥: {str(e)}")
        return False

def modify_lm_eval_configs():
    """ä¿®æ”¹lm_evalé…ç½®æ–‡ä»¶ä¸­çš„æ•°æ®é›†è·¯å¾„"""
    config_mappings = {
        "arc_easy.yaml": "./datasets/ai2_arc",
        "arc_challenge.yaml": "./datasets/ai2_arc", 
        "hellaswag.yaml": "./datasets/hellaswag",
        "lambada_openai.yaml": "./datasets/lambada_openai",
        "piqa.yaml": "./datasets/piqa",
        "winogrande.yaml": "./datasets/winogrande"
    }
    
    config_dir = Path("./datasets/lm_eval_configs/tasks")
    
    for config_file, dataset_path in config_mappings.items():
        config_path = config_dir / config_file
        if config_path.exists():
            try:
                # è¯»å–é…ç½®æ–‡ä»¶
                with open(config_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # ä¿®æ”¹æ•°æ®é›†è·¯å¾„
                # è¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“çš„é…ç½®æ–‡ä»¶æ ¼å¼è¿›è¡Œè°ƒæ•´
                # é€šå¸¸éœ€è¦å°†dataset_pathå­—æ®µä¿®æ”¹ä¸ºæœ¬åœ°è·¯å¾„
                modified_content = content.replace(
                    "dataset_path: null",
                    f"dataset_path: {dataset_path}"
                )
                
                # å†™å›æ–‡ä»¶
                with open(config_path, 'w', encoding='utf-8') as f:
                    f.write(modified_content)
                
                logger.info(f"ä¿®æ”¹é…ç½®æ–‡ä»¶: {config_file}")
                
            except Exception as e:
                logger.warning(f"ä¿®æ”¹é…ç½®æ–‡ä»¶å¤±è´¥ {config_file}: {str(e)}")

def create_dataset_info_file():
    """åˆ›å»ºæ•°æ®é›†ä¿¡æ¯æ–‡ä»¶"""
    info = {
        "download_time": datetime.now().isoformat(),
        "datasets": DATASETS_CONFIG,
        "usage": {
            "calibration_datasets": "ç”¨äºæ¨¡å‹æ ¡å‡†å’Œå›°æƒ‘åº¦è¯„ä¼°",
            "qa_datasets": "ç”¨äºå¸¸è¯†é—®ç­”ä»»åŠ¡è¯„ä¼°"
        },
        "notes": [
            "æ‰€æœ‰æ•°æ®é›†å·²ä¸‹è½½åˆ° ./datasets/ ç›®å½•",
            "lm_evalé…ç½®æ–‡ä»¶å·²å¤åˆ¶åˆ° ./datasets/lm_eval_configs/tasks/",
            "è¯·ç¡®ä¿åœ¨ä½¿ç”¨æ—¶è®¾ç½®æ­£ç¡®çš„æ•°æ®é›†è·¯å¾„"
        ]
    }
    
    with open("./datasets/dataset_info.json", 'w', encoding='utf-8') as f:
        json.dump(info, f, indent=2, ensure_ascii=False)
    
    logger.info("åˆ›å»ºæ•°æ®é›†ä¿¡æ¯æ–‡ä»¶: ./datasets/dataset_info.json")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='ä¸‹è½½FlatQuantæ‰€éœ€çš„æ•°æ®é›†')
    parser.add_argument('--calibration-only', action='store_true', 
                        help='åªä¸‹è½½æ ¡å‡†æ•°æ®é›†')
    parser.add_argument('--qa-only', action='store_true', 
                        help='åªä¸‹è½½é—®ç­”æ•°æ®é›†')
    parser.add_argument('--skip_lm_eval', action='store_true', 
                        help='è·³è¿‡lm_evalé…ç½®è®¾ç½®')
    parser.add_argument('--force', action='store_true', 
                        help='å¼ºåˆ¶é‡æ–°ä¸‹è½½å·²å­˜åœ¨çš„æ•°æ®é›†')
    
    args = parser.parse_args()
    
    print("ğŸš€ å¼€å§‹ä¸‹è½½FlatQuantæ‰€éœ€çš„æ•°æ®é›†")
    print("=" * 60)
    
    # åˆ›å»ºç›®å½•ç»“æ„
    create_directories()
    
    # ä¸‹è½½æ ¡å‡†æ•°æ®é›†
    if not args.qa_only:
        print("\nğŸ“Š ä¸‹è½½æ ¡å‡†å’Œå›°æƒ‘åº¦è¯„ä¼°æ•°æ®é›†...")
        calibration_success = 0
        calibration_total = len(DATASETS_CONFIG["calibration_datasets"])
        
        for dataset_name, dataset_info in DATASETS_CONFIG["calibration_datasets"].items():
            if download_dataset(dataset_info, "æ ¡å‡†", force=args.force):
                calibration_success += 1
    else:
        calibration_success = 0
        calibration_total = 0
    
    # ä¸‹è½½é—®ç­”æ•°æ®é›†
    if not args.calibration_only:
        print("\nâ“ ä¸‹è½½å¸¸è¯†é—®ç­”è¯„ä¼°æ•°æ®é›†...")
        qa_success = 0
        qa_total = len(DATASETS_CONFIG["qa_datasets"])
        
        for dataset_name, dataset_info in DATASETS_CONFIG["qa_datasets"].items():
            if download_dataset(dataset_info, "é—®ç­”", force=args.force):
                qa_success += 1
    else:
        qa_success = 0
        qa_total = 0
    
    # è®¾ç½®lm_evalé…ç½®
    lm_eval_success = False
    if not args.skip_lm_eval:
        print("\nâš™ï¸ è®¾ç½®lm_evalé…ç½®æ–‡ä»¶...")
        lm_eval_success = setup_lm_eval_configs()
    
    # åˆ›å»ºä¿¡æ¯æ–‡ä»¶
    create_dataset_info_file()
    
    # æ˜¾ç¤ºç»“æœ
    print("\n" + "=" * 60)
    print("ğŸ“‹ ä¸‹è½½ç»“æœæ±‡æ€»:")
    if calibration_total > 0:
        print(f"   æ ¡å‡†æ•°æ®é›†: {calibration_success}/{calibration_total} æˆåŠŸ")
    if qa_total > 0:
        print(f"   é—®ç­”æ•°æ®é›†: {qa_success}/{qa_total} æˆåŠŸ")
    if not args.skip_lm_eval:
        print(f"   lm_evalé…ç½®: {'âœ… æˆåŠŸ' if lm_eval_success else 'âŒ å¤±è´¥'}")
    
    total_success = calibration_success + qa_success
    total_datasets = calibration_total + qa_total
    
    if total_datasets == 0:
        print("\nâš ï¸ æ²¡æœ‰é€‰æ‹©è¦ä¸‹è½½çš„æ•°æ®é›†")
    elif total_success == total_datasets:
        print("\nğŸ‰ æ‰€æœ‰æ•°æ®é›†ä¸‹è½½å®Œæˆï¼")
        print("\nğŸ“ æ•°æ®é›†ä½ç½®:")
        print("   - æ ¡å‡†æ•°æ®é›†: ./datasets/")
        print("   - lm_evalé…ç½®: ./datasets/lm_eval_configs/tasks/")
        print("   - è¯¦ç»†ä¿¡æ¯: ./datasets/dataset_info.json")
        
        print("\nğŸ”§ ä½¿ç”¨è¯´æ˜:")
        print("   1. æ ¡å‡†æ•°æ®é›†å¯ç›´æ¥ç”¨äºFlatQuanté‡åŒ–")
        print("   2. é—®ç­”æ•°æ®é›†éœ€è¦é…åˆlm_evalä½¿ç”¨")
        print("   3. è¯·å‚è€ƒREADME.mdäº†è§£è¯¦ç»†ä½¿ç”¨æ–¹æ³•")
    else:
        print("\nâš ï¸ éƒ¨åˆ†æ•°æ®é›†ä¸‹è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œé”™è¯¯æ—¥å¿—")
        print("   æ‚¨å¯ä»¥ç¨åé‡æ–°è¿è¡Œæ­¤è„šæœ¬ä¸‹è½½å¤±è´¥çš„æ•°æ®é›†")

if __name__ == "__main__":
    main()
